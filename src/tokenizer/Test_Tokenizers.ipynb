{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41965fc4-101d-4c9e-92e2-32312a6de9b5",
   "metadata": {},
   "source": [
    "# Notebook to test various tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44241faa-5b3c-4207-94fa-a5a685ae3e79",
   "metadata": {},
   "source": [
    "## Basic BPE Tokenizer without any special tokens\n",
    "\n",
    "Let's test basic BPE tokenizer without any special tokens. <br>\n",
    "Let's first test with limited text (paragraph) and check if it's working correctly <br>\n",
    "Just to verify, let's try to compare the results with tiktoken tokenizer with gpt2 encoding <br>\n",
    "Let's start with vocab_size of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f651f339-fbba-4f98-b395-9dc9b3d166ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_bpe import BasicBPE\n",
    "from tiktoken import get_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ae0972-9917-4e0a-8de2-4b1b2ad83af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BasicBPE(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59873056-8ac4-42a5-935c-631e4a5b8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken_gpt2 = get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74858a24-70b7-4b8b-9417-ec9621d6205c",
   "metadata": {},
   "source": [
    "Let's test on first paragraph of Richard Feynman's wikipedia page(https://en.wikipedia.org/wiki/Richard_Feynman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8634f5c2-ff64-482e-bed2-86e2af427cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Richard Phillips Feynman (/ˈfaɪnmən/; May 11, 1918 – February 15, 1988) was an American theoretical physicist, known for his work in the path integral formulation of quantum mechanics, the theory of quantum electrodynamics, the physics of the superfluidity of supercooled liquid helium, as well as his work in particle physics for which he proposed the parton model. For his contributions to the development of quantum electrodynamics, Feynman received the Nobel Prize in Physics in 1965 jointly with Julian Schwinger and Shin'ichirō Tomonaga.\n",
    "\n",
    "Feynman developed a widely used pictorial representation scheme for the mathematical expressions describing the behavior of subatomic particles, which later became known as Feynman diagrams. During his lifetime, Feynman became one of the best-known scientists in the world. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, he was ranked the seventh-greatest physicist of all time.[1]\n",
    "\n",
    "He assisted in the development of the atomic bomb during World War II and became known to the wider public in the 1980s as a member of the Rogers Commission, the panel that investigated the Space Shuttle Challenger disaster. Along with his work in theoretical physics, Feynman has been credited with pioneering the field of quantum computing and introducing the concept of nanotechnology. He held the Richard C. Tolman professorship in theoretical physics at the California Institute of Technology.\n",
    "\n",
    "Feynman was a keen popularizer of physics through both books and lectures, including a 1959 talk on top-down nanotechnology called There's Plenty of Room at the Bottom and the three-volume publication of his undergraduate lectures, The Feynman Lectures on Physics. Feynman also became known through his autobiographical books Surely You're Joking, Mr. Feynman! and What Do You Care What Other People Think?, and books written about him such as Tuva or Bust! by Ralph Leighton and the biography Genius: The Life and Science of Richard Feynman by James Gleick.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0cff74-fc2e-48bb-8e5b-90b766a9e9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Richard Phillips Feynman (/ˈfaɪnmən/; May 11, 1918 – February 15, 1988) was an American theoretical physicist, known for his work in the path integral formulation of quantum mechanics, the theory of quantum electrodynamics, the physics of the superfluidity of supercooled liquid helium, as well as his work in particle physics for which he proposed the parton model. For his contributions to the development of quantum electrodynamics, Feynman received the Nobel Prize in Physics in 1965 jointly with Julian Schwinger and Shin'ichirō Tomonaga.\\n\\nFeynman developed a widely used pictorial representation scheme for the mathematical expressions describing the behavior of subatomic particles, which later became known as Feynman diagrams. During his lifetime, Feynman became one of the best-known scientists in the world. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, he was ranked the seventh-greatest physicist of all time.[1]\\n\\nHe assisted in the development of the atomic bomb during World War II and became known to the wider public in the 1980s as a member of the Rogers Commission, the panel that investigated the Space Shuttle Challenger disaster. Along with his work in theoretical physics, Feynman has been credited with pioneering the field of quantum computing and introducing the concept of nanotechnology. He held the Richard C. Tolman professorship in theoretical physics at the California Institute of Technology.\\n\\nFeynman was a keen popularizer of physics through both books and lectures, including a 1959 talk on top-down nanotechnology called There's Plenty of Room at the Bottom and the three-volume publication of his undergraduate lectures, The Feynman Lectures on Physics. Feynman also became known through his autobiographical books Surely You're Joking, Mr. Feynman! and What Do You Care What Other People Think?, and books written about him such as Tuva or Bust! by Ralph Leighton and the biography Genius: The Life and Science of Richard Feynman by James Gleick.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38613bf7-3d76-40ca-b258-bab831eef2f5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824a9179-c4c7-4c73-9335-eb1febf7694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281e5138-18fe-45fb-87af-981b12e8c721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (116, 104): 257,\n",
       " (110, 32): 258,\n",
       " (115, 32): 259,\n",
       " (105, 99): 260,\n",
       " (257, 256): 261,\n",
       " (100, 32): 262,\n",
       " (111, 114): 263,\n",
       " (111, 102): 264,\n",
       " (97, 110): 265,\n",
       " (264, 32): 266,\n",
       " (105, 110): 267,\n",
       " (44, 32): 268,\n",
       " (101, 114): 269,\n",
       " (97, 108): 270,\n",
       " (97, 116): 271,\n",
       " (97, 258): 272,\n",
       " (101, 99): 273,\n",
       " (101, 108): 274,\n",
       " (121, 110): 275,\n",
       " (121, 32): 276,\n",
       " (70, 101): 277,\n",
       " (104, 105): 278,\n",
       " (277, 275): 279,\n",
       " (279, 109): 280,\n",
       " (112, 104): 281,\n",
       " (121, 115): 282,\n",
       " (282, 260): 283,\n",
       " (115, 116): 284,\n",
       " (97, 114): 285,\n",
       " (280, 272): 286,\n",
       " (101, 262): 287,\n",
       " (267, 103): 288,\n",
       " (97, 259): 289,\n",
       " (105, 258): 290,\n",
       " (105, 111): 291,\n",
       " (101, 110): 292,\n",
       " (265, 262): 293,\n",
       " (114, 101): 294,\n",
       " (270, 32): 295,\n",
       " (281, 283): 296,\n",
       " (110, 111): 297,\n",
       " (116, 117): 298,\n",
       " (97, 109): 299,\n",
       " (46, 32): 300,\n",
       " (32, 261): 301,\n",
       " (32, 266): 302,\n",
       " (111, 109): 303,\n",
       " (278, 259): 304,\n",
       " (115, 268): 305,\n",
       " (111, 112): 306,\n",
       " (97, 32): 307,\n",
       " (49, 57): 308,\n",
       " (119, 258): 309,\n",
       " (104, 32): 310,\n",
       " (119, 105): 311,\n",
       " (111, 117): 312,\n",
       " (257, 101): 313,\n",
       " (116, 260): 314,\n",
       " (105, 284): 315,\n",
       " (107, 297): 316,\n",
       " (316, 309): 317,\n",
       " (102, 263): 318,\n",
       " (119, 263): 319,\n",
       " (32, 290): 320,\n",
       " (257, 32): 321,\n",
       " (103, 114): 322,\n",
       " (113, 117): 323,\n",
       " (109, 32): 324,\n",
       " (116, 111): 325,\n",
       " (111, 110): 326,\n",
       " (114, 105): 327,\n",
       " (292, 116): 328,\n",
       " (269, 32): 329,\n",
       " (288, 32): 330,\n",
       " (108, 105): 331,\n",
       " (313, 263): 332,\n",
       " (323, 265): 333,\n",
       " (333, 298): 334,\n",
       " (334, 324): 335,\n",
       " (273, 104): 336,\n",
       " (111, 100): 337,\n",
       " (296, 259): 338,\n",
       " (115, 117): 339,\n",
       " (100, 105): 340,\n",
       " (111, 111): 341,\n",
       " (104, 256): 342,\n",
       " (287, 261): 343,\n",
       " (291, 110): 344,\n",
       " (100, 101): 345,\n",
       " (115, 115): 346,\n",
       " (98, 273): 347,\n",
       " (347, 299): 348,\n",
       " (348, 256): 349,\n",
       " (111, 103): 350,\n",
       " (82, 260): 351,\n",
       " (351, 104): 352,\n",
       " (352, 285): 353,\n",
       " (353, 262): 354,\n",
       " (119, 289): 355,\n",
       " (332, 101): 356,\n",
       " (356, 314): 357,\n",
       " (357, 295): 358,\n",
       " (296, 315): 359,\n",
       " (304, 319): 360,\n",
       " (360, 107): 361,\n",
       " (361, 320): 362,\n",
       " (267, 116): 363,\n",
       " (271, 291): 364,\n",
       " (364, 258): 365,\n",
       " (266, 335): 366,\n",
       " (260, 305): 367,\n",
       " (114, 337): 368,\n",
       " (108, 117): 369,\n",
       " (112, 285): 370,\n",
       " (108, 256): 371,\n",
       " (117, 116): 372,\n",
       " (345, 118): 373,\n",
       " (373, 274): 374,\n",
       " (374, 306): 375,\n",
       " (80, 104): 376,\n",
       " (376, 283): 377,\n",
       " (311, 321): 378,\n",
       " (10, 10): 379,\n",
       " (311, 100): 380,\n",
       " (115, 99): 381,\n",
       " (288, 301): 382,\n",
       " (98, 101): 383,\n",
       " (108, 101): 384,\n",
       " (349, 317): 385,\n",
       " (117, 114): 386,\n",
       " (116, 105): 387,\n",
       " (109, 101): 388,\n",
       " (256, 266): 389,\n",
       " (108, 100): 390,\n",
       " (98, 276): 391,\n",
       " (270, 108): 392,\n",
       " (32, 293): 393,\n",
       " (271, 32): 394,\n",
       " (101, 258): 395,\n",
       " (111, 116): 396,\n",
       " (336, 297): 397,\n",
       " (397, 108): 398,\n",
       " (398, 350): 399,\n",
       " (98, 341): 400,\n",
       " (400, 107): 401,\n",
       " (401, 259): 402,\n",
       " (273, 298): 403,\n",
       " (403, 294): 404,\n",
       " (322, 97): 405,\n",
       " (276, 49): 406,\n",
       " (268, 308): 407,\n",
       " (318, 32): 408,\n",
       " (261, 112): 409,\n",
       " (117, 108): 410,\n",
       " (367, 261): 411,\n",
       " (274, 273): 412,\n",
       " (412, 116): 413,\n",
       " (413, 368): 414,\n",
       " (414, 275): 415,\n",
       " (415, 299): 416,\n",
       " (339, 112): 417,\n",
       " (417, 269): 418,\n",
       " (340, 116): 419,\n",
       " (276, 266): 420,\n",
       " (104, 274): 421,\n",
       " (105, 117): 422,\n",
       " (370, 314): 423,\n",
       " (119, 104): 424,\n",
       " (424, 260): 425,\n",
       " (425, 310): 426,\n",
       " (112, 114): 427,\n",
       " (325, 258): 428,\n",
       " (263, 32): 429,\n",
       " (99, 326): 430,\n",
       " (327, 98): 431,\n",
       " (344, 259): 432,\n",
       " (325, 301): 433,\n",
       " (375, 109): 434,\n",
       " (434, 328): 435,\n",
       " (435, 302): 436,\n",
       " (101, 105): 437,\n",
       " (274, 32): 438,\n",
       " (377, 259): 439,\n",
       " (83, 99): 440,\n",
       " (83, 104): 441,\n",
       " (32, 84): 442,\n",
       " (46, 379): 443,\n",
       " (443, 286): 444,\n",
       " (274, 276): 445,\n",
       " (112, 294): 446,\n",
       " (109, 256): 447,\n",
       " (260, 295): 448,\n",
       " (271, 303): 449,\n",
       " (449, 260): 450,\n",
       " (450, 32): 451,\n",
       " (115, 300): 452,\n",
       " (386, 330): 453,\n",
       " (387, 388): 454,\n",
       " (290, 261): 455,\n",
       " (319, 390): 456,\n",
       " (307, 308): 457,\n",
       " (57, 32): 458,\n",
       " (111, 108): 459,\n",
       " (100, 330): 460,\n",
       " (87, 263): 461,\n",
       " (101, 284): 462,\n",
       " (72, 256): 463,\n",
       " (112, 117): 464,\n",
       " (464, 98): 465,\n",
       " (465, 108): 466,\n",
       " (466, 260): 467,\n",
       " (269, 302): 468,\n",
       " (110, 265): 469,\n",
       " (469, 396): 470,\n",
       " (470, 399): 471,\n",
       " (271, 301): 472,\n",
       " (257, 114): 473,\n",
       " (473, 312): 474,\n",
       " (474, 103): 475,\n",
       " (475, 310): 476,\n",
       " (108, 404): 477,\n",
       " (477, 305): 478,\n",
       " (111, 258): 479,\n",
       " (84, 104): 480,\n",
       " (111, 32): 481,\n",
       " (98, 291): 482,\n",
       " (482, 405): 483,\n",
       " (483, 281): 484,\n",
       " (89, 312): 485,\n",
       " (87, 104): 486,\n",
       " (486, 394): 487,\n",
       " (354, 80): 488,\n",
       " (488, 278): 489,\n",
       " (489, 108): 490,\n",
       " (490, 331): 491,\n",
       " (491, 112): 492,\n",
       " (492, 259): 493,\n",
       " (493, 286): 494,\n",
       " (494, 40): 495,\n",
       " (495, 47): 496,\n",
       " (496, 203): 497,\n",
       " (497, 136): 498,\n",
       " (498, 102): 499}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merges\n",
    "bpe.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e56d5e6-ac07-4380-990d-6ea8766ba8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'e ',\n",
       " 257: b'th',\n",
       " 258: b'n ',\n",
       " 259: b's ',\n",
       " 260: b'ic',\n",
       " 261: b'the ',\n",
       " 262: b'd ',\n",
       " 263: b'or',\n",
       " 264: b'of',\n",
       " 265: b'an',\n",
       " 266: b'of ',\n",
       " 267: b'in',\n",
       " 268: b', ',\n",
       " 269: b'er',\n",
       " 270: b'al',\n",
       " 271: b'at',\n",
       " 272: b'an ',\n",
       " 273: b'ec',\n",
       " 274: b'el',\n",
       " 275: b'yn',\n",
       " 276: b'y ',\n",
       " 277: b'Fe',\n",
       " 278: b'hi',\n",
       " 279: b'Feyn',\n",
       " 280: b'Feynm',\n",
       " 281: b'ph',\n",
       " 282: b'ys',\n",
       " 283: b'ysic',\n",
       " 284: b'st',\n",
       " 285: b'ar',\n",
       " 286: b'Feynman ',\n",
       " 287: b'ed ',\n",
       " 288: b'ing',\n",
       " 289: b'as ',\n",
       " 290: b'in ',\n",
       " 291: b'io',\n",
       " 292: b'en',\n",
       " 293: b'and ',\n",
       " 294: b're',\n",
       " 295: b'al ',\n",
       " 296: b'physic',\n",
       " 297: b'no',\n",
       " 298: b'tu',\n",
       " 299: b'am',\n",
       " 300: b'. ',\n",
       " 301: b' the ',\n",
       " 302: b' of ',\n",
       " 303: b'om',\n",
       " 304: b'his ',\n",
       " 305: b's, ',\n",
       " 306: b'op',\n",
       " 307: b'a ',\n",
       " 308: b'19',\n",
       " 309: b'wn ',\n",
       " 310: b'h ',\n",
       " 311: b'wi',\n",
       " 312: b'ou',\n",
       " 313: b'the',\n",
       " 314: b'tic',\n",
       " 315: b'ist',\n",
       " 316: b'kno',\n",
       " 317: b'known ',\n",
       " 318: b'for',\n",
       " 319: b'wor',\n",
       " 320: b' in ',\n",
       " 321: b'th ',\n",
       " 322: b'gr',\n",
       " 323: b'qu',\n",
       " 324: b'm ',\n",
       " 325: b'to',\n",
       " 326: b'on',\n",
       " 327: b'ri',\n",
       " 328: b'ent',\n",
       " 329: b'er ',\n",
       " 330: b'ing ',\n",
       " 331: b'li',\n",
       " 332: b'theor',\n",
       " 333: b'quan',\n",
       " 334: b'quantu',\n",
       " 335: b'quantum ',\n",
       " 336: b'ech',\n",
       " 337: b'od',\n",
       " 338: b'physics ',\n",
       " 339: b'su',\n",
       " 340: b'di',\n",
       " 341: b'oo',\n",
       " 342: b'he ',\n",
       " 343: b'ed the ',\n",
       " 344: b'ion',\n",
       " 345: b'de',\n",
       " 346: b'ss',\n",
       " 347: b'bec',\n",
       " 348: b'becam',\n",
       " 349: b'became ',\n",
       " 350: b'og',\n",
       " 351: b'Ric',\n",
       " 352: b'Rich',\n",
       " 353: b'Richar',\n",
       " 354: b'Richard ',\n",
       " 355: b'was ',\n",
       " 356: b'theore',\n",
       " 357: b'theoretic',\n",
       " 358: b'theoretical ',\n",
       " 359: b'physicist',\n",
       " 360: b'his wor',\n",
       " 361: b'his work',\n",
       " 362: b'his work in ',\n",
       " 363: b'int',\n",
       " 364: b'atio',\n",
       " 365: b'ation ',\n",
       " 366: b'of quantum ',\n",
       " 367: b'ics, ',\n",
       " 368: b'rod',\n",
       " 369: b'lu',\n",
       " 370: b'par',\n",
       " 371: b'le ',\n",
       " 372: b'ut',\n",
       " 373: b'dev',\n",
       " 374: b'devel',\n",
       " 375: b'develop',\n",
       " 376: b'Ph',\n",
       " 377: b'Physic',\n",
       " 378: b'with ',\n",
       " 379: b'\\n\\n',\n",
       " 380: b'wid',\n",
       " 381: b'sc',\n",
       " 382: b'ing the ',\n",
       " 383: b'be',\n",
       " 384: b'le',\n",
       " 385: b'became known ',\n",
       " 386: b'ur',\n",
       " 387: b'ti',\n",
       " 388: b'me',\n",
       " 389: b'e of ',\n",
       " 390: b'ld',\n",
       " 391: b'by ',\n",
       " 392: b'all',\n",
       " 393: b' and ',\n",
       " 394: b'at ',\n",
       " 395: b'en ',\n",
       " 396: b'ot',\n",
       " 397: b'echno',\n",
       " 398: b'echnol',\n",
       " 399: b'echnolog',\n",
       " 400: b'boo',\n",
       " 401: b'book',\n",
       " 402: b'books ',\n",
       " 403: b'ectu',\n",
       " 404: b'ecture',\n",
       " 405: b'gra',\n",
       " 406: b'y 1',\n",
       " 407: b', 19',\n",
       " 408: b'for ',\n",
       " 409: b'the p',\n",
       " 410: b'ul',\n",
       " 411: b'ics, the ',\n",
       " 412: b'elec',\n",
       " 413: b'elect',\n",
       " 414: b'electrod',\n",
       " 415: b'electrodyn',\n",
       " 416: b'electrodynam',\n",
       " 417: b'sup',\n",
       " 418: b'super',\n",
       " 419: b'dit',\n",
       " 420: b'y of ',\n",
       " 421: b'hel',\n",
       " 422: b'iu',\n",
       " 423: b'partic',\n",
       " 424: b'wh',\n",
       " 425: b'whic',\n",
       " 426: b'which ',\n",
       " 427: b'pr',\n",
       " 428: b'ton ',\n",
       " 429: b'or ',\n",
       " 430: b'con',\n",
       " 431: b'rib',\n",
       " 432: b'ions ',\n",
       " 433: b'to the ',\n",
       " 434: b'developm',\n",
       " 435: b'development',\n",
       " 436: b'development of ',\n",
       " 437: b'ei',\n",
       " 438: b'el ',\n",
       " 439: b'Physics ',\n",
       " 440: b'Sc',\n",
       " 441: b'Sh',\n",
       " 442: b' T',\n",
       " 443: b'.\\n\\n',\n",
       " 444: b'.\\n\\nFeynman ',\n",
       " 445: b'ely ',\n",
       " 446: b'pre',\n",
       " 447: b'me ',\n",
       " 448: b'ical ',\n",
       " 449: b'atom',\n",
       " 450: b'atomic',\n",
       " 451: b'atomic ',\n",
       " 452: b's. ',\n",
       " 453: b'uring ',\n",
       " 454: b'time',\n",
       " 455: b'in the ',\n",
       " 456: b'world',\n",
       " 457: b'a 19',\n",
       " 458: b'9 ',\n",
       " 459: b'ol',\n",
       " 460: b'ding ',\n",
       " 461: b'Wor',\n",
       " 462: b'est',\n",
       " 463: b'He ',\n",
       " 464: b'pu',\n",
       " 465: b'pub',\n",
       " 466: b'publ',\n",
       " 467: b'public',\n",
       " 468: b'er of ',\n",
       " 469: b'nan',\n",
       " 470: b'nanot',\n",
       " 471: b'nanotechnolog',\n",
       " 472: b'at the ',\n",
       " 473: b'thr',\n",
       " 474: b'throu',\n",
       " 475: b'throug',\n",
       " 476: b'through ',\n",
       " 477: b'lecture',\n",
       " 478: b'lectures, ',\n",
       " 479: b'on ',\n",
       " 480: b'Th',\n",
       " 481: b'o ',\n",
       " 482: b'bio',\n",
       " 483: b'biogra',\n",
       " 484: b'biograph',\n",
       " 485: b'You',\n",
       " 486: b'Wh',\n",
       " 487: b'What ',\n",
       " 488: b'Richard P',\n",
       " 489: b'Richard Phi',\n",
       " 490: b'Richard Phil',\n",
       " 491: b'Richard Philli',\n",
       " 492: b'Richard Phillip',\n",
       " 493: b'Richard Phillips ',\n",
       " 494: b'Richard Phillips Feynman ',\n",
       " 495: b'Richard Phillips Feynman (',\n",
       " 496: b'Richard Phillips Feynman (/',\n",
       " 497: b'Richard Phillips Feynman (/\\xcb',\n",
       " 498: b'Richard Phillips Feynman (/\\xcb\\x88',\n",
       " 499: b'Richard Phillips Feynman (/\\xcb\\x88f'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary\n",
    "bpe.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d6766-8cec-440f-a351-a4b5e6fb6222",
   "metadata": {},
   "source": [
    "Let's try very simple encode and decode example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c2989e-348a-4f47-93dd-0960eb8371bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = bpe.encode(\"Feynman served as doctoral advisor to 30 students.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d48cf0-9163-4b82-9269-9edb9895e11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[286,\n",
       " 115,\n",
       " 269,\n",
       " 118,\n",
       " 287,\n",
       " 289,\n",
       " 100,\n",
       " 111,\n",
       " 99,\n",
       " 116,\n",
       " 263,\n",
       " 295,\n",
       " 97,\n",
       " 100,\n",
       " 118,\n",
       " 105,\n",
       " 115,\n",
       " 429,\n",
       " 325,\n",
       " 32,\n",
       " 51,\n",
       " 48,\n",
       " 32,\n",
       " 284,\n",
       " 117,\n",
       " 100,\n",
       " 328,\n",
       " 115,\n",
       " 46]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc532fe4-2fb7-432e-bbfa-08b125cc3f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feynman served as doctoral advisor to 30 students.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode\n",
    "bpe.decode(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25c3c6f5-fb9e-4a63-89cc-19c52f0ecbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(text, ids, enc):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"My Enocder: \")\n",
    "    for i, idx in enumerate(ids):\n",
    "        try:\n",
    "            print(f'{i+1} : {enc.vocab[idx]}')\n",
    "        except:\n",
    "            print(f'{i+1} : UNK')\n",
    "    \n",
    "    print(\"TikToken gpt2 encoder\")\n",
    "    for i, idx in enumerate(tiktoken_gpt2.encode(text)):\n",
    "        try:\n",
    "            print(f'{i+1} : {tiktoken_gpt2.decode_single_token_bytes(idx)}')\n",
    "        except:\n",
    "            print(f'{i+1} : UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cb6401c-29d6-4c64-b32e-acba000da9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Feynman served as doctoral advisor to 30 students.\n",
      "My Enocder: \n",
      "1 : b'Feynman '\n",
      "2 : b's'\n",
      "3 : b'er'\n",
      "4 : b'v'\n",
      "5 : b'ed '\n",
      "6 : b'as '\n",
      "7 : b'd'\n",
      "8 : b'o'\n",
      "9 : b'c'\n",
      "10 : b't'\n",
      "11 : b'or'\n",
      "12 : b'al '\n",
      "13 : b'a'\n",
      "14 : b'd'\n",
      "15 : b'v'\n",
      "16 : b'i'\n",
      "17 : b's'\n",
      "18 : b'or '\n",
      "19 : b'to'\n",
      "20 : b' '\n",
      "21 : b'3'\n",
      "22 : b'0'\n",
      "23 : b' '\n",
      "24 : b'st'\n",
      "25 : b'u'\n",
      "26 : b'd'\n",
      "27 : b'ent'\n",
      "28 : b's'\n",
      "29 : b'.'\n",
      "TikToken gpt2 encoder\n",
      "1 : b'Fe'\n",
      "2 : b'yn'\n",
      "3 : b'man'\n",
      "4 : b' served'\n",
      "5 : b' as'\n",
      "6 : b' doctoral'\n",
      "7 : b' advisor'\n",
      "8 : b' to'\n",
      "9 : b' 30'\n",
      "10 : b' students'\n",
      "11 : b'.'\n"
     ]
    }
   ],
   "source": [
    "# just to compare with tiktoken tokenizer, let's just see how the sentence is getting tokenized\n",
    "sent = \"Feynman served as doctoral advisor to 30 students.\"\n",
    "\n",
    "compare(sent, enc, bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f7957-cd1e-4a23-9634-1e2ac0c36388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29c85f9-87cb-4770-a8a1-d014c9603669",
   "metadata": {},
   "source": [
    "#### Let's try a bigger text now\n",
    "\n",
    "Let's encode another paragraph from Feynman's wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b2818-34c6-4a32-bcc3-80c3a7322d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"Surely You're Joking, Mr. Feynman!\n",
    "In the 1960s, Feynman began thinking of writing an autobiography, and he began granting interviews to historians. In the 1980s, working with Ralph Leighton (Robert Leighton's son), he recorded chapters on audio tape that Ralph transcribed. The book was published in 1985 as Surely You're Joking, Mr. Feynman! and became a best-seller.[174]\n",
    "\n",
    "Gell-Mann was upset by Feynman's account in the book of the weak interaction work, and threatened to sue, resulting in a correction being inserted in later editions.[175] This incident was just the latest provocation in decades of bad feeling between the two scientists. Gell-Mann often expressed frustration at the attention Feynman received;[176] he remarked: \"[Feynman] was a great scientist, but he spent a great deal of his effort generating anecdotes about himself.\"[177]\n",
    "\n",
    "Feynman has been criticized for a chapter in the book entitled \"You Just Ask Them?\", where he describes how he learned to seduce women at a bar he went to in the summer of 1946. A mentor taught him to ask a woman if she would sleep with him before buying her anything. He describes seeing women at the bar as \"bitches\" in his thoughts, and tells a story of how he told a woman named Ann that she was \"worse than a whore\" after Ann persuaded him to buy her sandwiches by telling him he could eat them at her place, but then, after he bought them, saying they actually could not eat together because another man was coming over. Later on that same evening, Ann returned to the bar to take Feynman to her place.[178][179][180] Feynman states at the end of the chapter that this behaviour was not typical of him: \\\"So it worked even with an ordinary girl! But no matter how effective the lesson was, I never really used it after that. I didn't enjoy doing it that way. But it was interesting to know that things worked much differently from how I was brought up.\\\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4985e-92ff-41a0-bcd8-e2fb85f8efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179ceee-98d5-479e-ac29-63f2ab37edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = bpe.encode(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeec326-63e9-47b5-9d77-37721666e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode\n",
    "bpe.decode(enc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6230c27-b759-439a-bcac-5b8901ea7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert text2 == bpe.decode(enc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11f0b6-fa92-4028-acae-c44685854d95",
   "metadata": {},
   "source": [
    "So the encoding worked well with limited text. <br>\n",
    "Now let's try how the tokenizer works on python code <br>\n",
    "let's check how it works on its own code :) <br>\n",
    "\n",
    "```\n",
    "class BasicBPE(Tokenizer):\n",
    "    def __init__(self, vocab_size: int=32000) -> None:\n",
    "        super().__init__()\n",
    "        assert vocab_size > 256, f\"Vocabulary size should be more than 256.\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_merges = vocab_size - 256\n",
    "\n",
    "    def train(self, text: str) -> None:\n",
    "        text_bytes = bytes(text, encoding=\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        for i in range(self.num_merges):\n",
    "            stats = get_stats(ids)\n",
    "            pair = stats.most_common(1)[0][0]\n",
    "            idx = 256 + i\n",
    "            ids = merge(ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "\n",
    "        self.build_vocab()\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "\n",
    "        while(len(ids) >= 2):\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d8b8b-45d7-4edd-aa7d-43b4da1bee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_code = \"\"\"\n",
    "class BasicBPE(Tokenizer):\n",
    "    def __init__(self, vocab_size: int=32000) -> None:\n",
    "        super().__init__()\n",
    "        assert vocab_size > 256, f\"Vocabulary size should be more than 256.\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_merges = vocab_size - 256\n",
    "\n",
    "    def train(self, text: str) -> None:\n",
    "        text_bytes = bytes(text, encoding=\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        for i in range(self.num_merges):\n",
    "            stats = get_stats(ids)\n",
    "            pair = stats.most_common(1)[0][0]\n",
    "            idx = 256 + i\n",
    "            ids = merge(ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "\n",
    "        self.build_vocab()\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "\n",
    "        while(len(ids) >= 2):\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d4fc3-8467-4029-be4d-8b050bfbb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode\n",
    "my_code_enc = bpe.encode(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e07044-a141-42ed-90e7-9e3f26c4ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's decode and see what we get\n",
    "bpe.decode(my_code_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b6ae7-bf8e-453c-8d3c-7ad62b9621b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77066f3b-2c0e-4c01-8201-a585c101830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the code is tokenized\n",
    "print(\"Tokens: \")\n",
    "for i, idx in enumerate(my_code_enc):\n",
    "    try:\n",
    "        print(f'{i+1} : {bpe.vocab[idx]}')\n",
    "    except:\n",
    "        print(f'{i+1} : UNK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bf55d-cc80-40e7-9ab8-32cd93e759a4",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "1. Tokenizer doesn't have the notion of language keywords like self, def, class etc and tokenizes each character separately.\n",
    "2. Tokenizer doesn't have the notion of tab\n",
    "3. Will adding more python code be better for tokenizer training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735aa7d-0548-47f9-81d0-d238db5eccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the tokenizer on code and let's see how the tokenizer behaves on text\n",
    "# Read directory and find out python files.\n",
    "# Keep one for the testing and read all other python files into single text string\n",
    "import glob\n",
    "\n",
    "all_py = glob.glob(\"../**/*.py\", recursive=True)\n",
    "train_py, test_py = all_py[:-1], all_py[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f3fa4-7488-45ca-92c9-304a8ad77773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py, test_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bcb8f-c8d9-4eff-9da5-a511f4bfb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"\"\n",
    "\n",
    "for fp in train_py:\n",
    "    with open(fp, 'r') as f:\n",
    "        train_text += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23be2ec-2afe-422e-8b23-117e6d04ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\n",
    "\n",
    "with open(test_py, 'r') as f:\n",
    "    test_text += f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36179a8-d447-4947-bd80-fdfc93bbea99",
   "metadata": {},
   "source": [
    "Now let's train a simple tokenizer with vocab size = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81be87-c818-42e6-a1d1-9b173d12bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_code = BasicBPE(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbfefd-804b-42f9-9069-bc36c00f28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_code.train(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d80393-ad4c-45f0-bbac-c6fe5d5e0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode the test text now\n",
    "code_enc = bpe_code.encode(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e37da4-7bdc-42ba-90f2-ea6752469e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_code.decode(code_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ae081-a6e1-40f2-a3cc-4d0d247a1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the code is tokenized\n",
    "print(\"Tokens: \")\n",
    "for i, idx in enumerate(code_enc):\n",
    "    try:\n",
    "        print(f'{i+1} : {bpe_code.vocab[idx]}')\n",
    "    except:\n",
    "        print(f'{i+1} : UNK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a666d-4306-4f2c-a2b5-96fa1f9050ca",
   "metadata": {},
   "source": [
    "The tokenizer already starts to tokenize some keywords like import, return, def etc as a whole. <br>\n",
    "Still there are some weird tokens :- \") -> None:\\n        super().__init__()\\n \" <br>       \n",
    "With more data to train on, the tokenizer may become better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17157ecc-3253-429e-a4e1-f985a8c06bc5",
   "metadata": {},
   "source": [
    "#### Let's try the tokenizer on simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d955e0-8dc6-4fa9-8a7b-408384bfd1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = bpe_code.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc879c-ba26-46d1-904b-b104fe015565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the code is tokenized\n",
    "print(\"Tokens: \")\n",
    "for i, idx in enumerate(enc_text):\n",
    "    try:\n",
    "        print(f'{i+1} : {bpe_code.vocab[idx]}')\n",
    "    except:\n",
    "        print(f'{i+1} : UNK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15721e6-ad69-4a41-b5a9-5d765483e86a",
   "metadata": {},
   "source": [
    "Mostly the text is tokenized at the character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e060c82-746e-4e41-8815-9e102979cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_code.decode(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903d4fc-a7e3-4462-874e-d3893745adbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
